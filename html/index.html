
<html slick-uniqueid="3">

<head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">




<title>ENGG5104 Final Project: Image Classification</title>


</head>

<body bgcolor="#FFFFFF">



<h1 align="center"><font face="Arial"><strong>ENGG5104 Final Project: Image Classification</strong></font></h1>





<h2 align="left"><font face="Arial"><u>Goal</u></font></h2>

The goal of the project is to implement an image classification system. The main task is to train image classifiers on the training set, and do testing on new images. 

<h2 align="left"><font face="Arial"><u>Bag of Features</u></font></h2>
 
<div align="left">
<a href="./BOW.jpg" ><img  src="./BOW.png" alt="BOW" width="800" height="600" /></a>
</div>
<br> <br> 
The bag of features model is used widely in image classification. 
<br> <br> 
<h2 align="left"><font face="Arial"><u>Spatial Pyramid Matching</u></font></h2>
<div align="left">
<a href="./SPM.jpg" ><img  src="./SPM.png" alt="SPM" width="800" height="520" /></a>
</div>
<br> <br>
The spatial pyramid is an extension of the bag of features which locally orderless representing images at several leves of resolution. 
<br> <br>
<h2 align="left"><font face="Arial"><u>Image Classification System</u></font></h2>

<h3><font face="Arial">STEP 1: Feature extraction</font></h3>
In this project, dense sampling of SIFT descriptors is chosen as the feature extraction method.
<h3><font face="Arial">STEP 2: Codebook Generation</font></h3>
K-means clustering is applied on the feature descriptors to learn the codebook.
<h3><font face="Arial">STEP 3: Spatial pyramid matching</font></h3>
A spatial pyramid matching methods with three layer is used in this project.
<h3><font face="Arial">STEP 4: Image representation</font></h3>
Two image representation methods are implymented in the frameworks: (1) descriptor quantization; (2) approximated LLC.
<h3><font face="Arial">STEP 5: Classifier learning</font></h3>
Support Vector Machine with (1) linear kernel and (2) Radial kernel are implymented in the frameworks.


 
 
<h2 align="left"><font face="Arial"><u>Experiment Results</u></font></h2>
 

<h3>-Single level v.s. Spatial pyramid matching</h3>
Fixed Parameter:

1024 codebook size + VQ + linear SVM
<tr valign="top">
	<td colspan="2" width="1040">
		<div class="wpmd">
			<div>
				<br>
			</div>
			<div align="center">
				<img border="0" hspace="1" vspace="1" src="./Dimetrodon.jpg" width="300" height="150">
				<font class="ws16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
				<img border="0" hspace="1" vspace="1" src="./Dimetrodon_f.jpg" width="300" height="150">
			</div>
			<div>
				<br>
			</div>
			<div align="center">
				<font class="ws16">Single level: 30%</font>
				<font class="ws16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
				<font class="ws16">Spatial pyramid matching: 38.5%</font>
			</div>
			<div>
				<br>
			</div>
		</div>
	</td>
</tr>
From the result we can find that with the combination of different levels, the performance is rapidly improved. SPM is expecially useful on the topic of scene recogntion because the location information remain.

<h3>-Different codebook size</h3>
Fixed Parameter:

VQ + SPM + linear SVM
<tr valign="top">
	<td colspan="2" width="1040">
		<div class="wpmd">
			<div>
				<br>
			</div>
			<div align="center">
				<img border="0" hspace="1" vspace="1" src="./f2.jpg" width="300" height="150">
				<font class="ws16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
				<img border="0" hspace="1" vspace="1" src="./Dimetrodon_f.jpg" width="300" height="150">
			</div>
			<div>
				<br>
			</div>
			<div align="center">
				<font class="ws16">512: 37.5%</font>
				<font class="ws16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
				<font class="ws16">1024: 38.5%</font>
			</div>
			<div>
				<br>
			</div>
		</div>
	</td>
</tr>
From the result we can find that the size also have some influence on the performance. Smaller size is faster, but larger size can keep more kinds of feature clusters.
<h3>-Linear kernel v.s. Radial kernel</h3>
Fixed Parameter:

1024 codebook size + VQ + SPM 
<br>
PS: The parameters of radial kernels are trained automatically and the one with best performance is gamma=0.0078125, C=512.
<tr valign="top">
	<td colspan="2" width="1040">
		<div class="wpmd">
			<div>
				<br>
			</div>
			<div align="center">
				<img border="0" hspace="1" vspace="1" src="./Dimetrodon_f.jpg" width="300" height="150">
				<font class="ws16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
				<img border="0" hspace="1" vspace="1" src="./f3.jpg" width="300" height="150">
			</div>
			<div>
				<br>
			</div>
			<div align="center">
				<font class="ws16">Linear kernel: 38.5%</font>
				<font class="ws16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
				<font class="ws16">Radial kernel: 34%</font>
			</div>
			<div>
				<br>
			</div>
		</div>
	</td>
</tr>  
From the result we can find that although in training step, the radial kernel SVM might fit the data better, it might cause overfitting and might not have a better accuracy on evaluation step. Although VQ is nonlinear, linear SVM still get a better performance than radial kernel SVM.
<h3>-VQ v.s. LLC</h3>
Fixed Parameter:

1024 codebook size + SPM + linear SVM
<br>
PS: The parameters of radial kernels are trained automatically and the one with best performance is gamma=0.0078125, C=512.
<tr valign="top">
	<td colspan="2" width="1040">
		<div class="wpmd">
			<div>
				<br>
			</div>
			<div align="center">
				<img border="0" hspace="1" vspace="1" src="./Dimetrodon_f.jpg" width="300" height="150">
				<font class="ws16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
				<img border="0" hspace="1" vspace="1" src="./f4.jpg" width="300" height="150">
			</div>
			<div>
				<br>
			</div>
			<div align="center">
				<font class="ws16">VQ: 38.5%</font>
				<font class="ws16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
				<font class="ws16">LLC: 23.5%</font>
			</div>
			<div>
				<br>
			</div>
		</div>
	</td>
</tr>  

It is unexpectedly that LLC is not better that VQ and has strong bias on class 1 and 2. Maybe there are some errors on LLC programing codes that I haven't found.

<h2 align="left"><font face="Arial"><u>Final System</u></font></h2>
The final classifier is generated by dense SIFT, 1024 codebook size with k-means clustering, SPM, VQ with mean pooling, and linear SVM. 
<br><br><br>
</body>



</html>
